# 01. 데이터 분석과 AI에서 통계의 중요성
1. **평가 지표** : 통계적 지표로 모델의 성능을 측정  
2. **가설 검정** : 모델간 성능 차이가 유의미한지 확인하기 위해 t-검정이나 p-값을 계산  
3. **분산분석** : 여러 모델의 성능 차이를 비교  

> 통계적인 지식 없이는 결과를 비판적으로 해석하거나 모델을 올바르게 설계하기 어렵다



# 02. 기본 개념


## 1. CLT (중심극한정리)
> 모집단의 분포에 상관없이 크기가 N인 표본을 반복적으로 추출하여 그 표본 평균을 계산할 때, N이 충분히 크다면 표본 평균의 분포는 정규분포에 가까워진다
 
<img width="524" height="229" alt="image" src="https://github.com/user-attachments/assets/dc40c77c-eca1-459f-9e47-d9e0474cbc4b" />


### 1) CLT의 이점
1. **정규분포의 보편성** – 다양한 실제 데이터를 분석할 때 정규분포 근사가 가능하게 한다
2. **샘플 기반 추론** – 모집단의 분포를 모르더라도 표본평균을 통해서 추론 가능
3. **기계학습과 AI 활용** – Cross-validation의 이론적 근거  

> Cross-validation 결과 평균은 CLT에 따라 정규분포로 근사 가능 → 신뢰구간 추정 가능

### 2) CLT 조건
1. Independence – 표본 데이터는 서로 독립적  
2. Identical Distribution – 동일한 분포를 따라야 함  
3. Sample Size – 표본 크기가 충분히 커야 함 (n ≥ 30 권장)  
4. Finite Variance – 모집단의 분산이 유한해야 함  
5. 표본 크기와 모집단의 비율 – 모집단이 유한한 경우, 표본은 모집단의 10% 이하  


## 2. Bayes’ Theorem (베이즈정리)
> 조건부 확률을 이용해 새로운 정보가 주어졌을 때 특정 사건의 확률을 갱신하는 방법

<img width="412" height="139" alt="image" src="https://github.com/user-attachments/assets/b1ac9b0c-5112-4ffb-90fd-4ab8f4df8b12" />

<img width="134" height="64" alt="image" src="https://github.com/user-attachments/assets/66caae45-14ac-446c-a8ce-5446187626c0" />

### 1) 사후확률 : 데이터 B 확인 후 가설 A의 확률

<img width="132" height="49" alt="image" src="https://github.com/user-attachments/assets/1773b37c-f996-4a03-8ef9-30509913609e" />

### 2) 우도 : 가설 A가 맞다고 가정했을 때 데이터 B가 나올 확률

<img width="98" height="56" alt="image" src="https://github.com/user-attachments/assets/6e70a04e-cdc5-4128-9746-d3da5190e2fb" />

### 3) 사전확률 : 데이터 B 보기 전 A의 확률

<img width="106" height="55" alt="image" src="https://github.com/user-attachments/assets/59efbdec-510c-4ab0-b663-3018133cee6e" />

### 4) 정규화 상수 : 데이터 B가 관측될 확률  

### 5) ML에서의 활용
1. 점진적 업데이트 : 전체 데이터를 다시 학습하지 않고도 예측력 유지가능
2. MAP, BIC 계산 : 모두 베이즈 정리를 활용해 계산이 이루어짐
3. Naive Bayes 분류기 : 베이즈 정리를 이용해 가장 가능성 높은 클래스를 예측하는 가능성 높은 클래스를 예측하는 간단한 방법

---

## 3. 가설 검정

### 1)가설설정 
1. 귀무가설 (H0)** : 주장하려는 바의 반대 내용  
2. 대립가설 (H1)** : 주장하려는 바에 부합하는 내용  

### 2)검정 통계량
- 표본에서 관측된 값이 귀무가설 하에서 얼마나 극단적인지를 정량화한 지표
- 
1. Z-통계량 : 모분산을 아는 경우 모집단 평균 비교를 위해서 사용 → 표준정규분포 N(0,1)을 사용함 
2. t-통계량 : 모분산을 모르는 경우 모집단 평균 비교를 위해서 사용 → t-분포 (df=n-1)을 사용함
3. F-통계량 : 두 집단 이상의 분산비교 또는 분산분석(ANOVA) 을 위해서 사용 → F-분포(df1,df2)를 사용함

### 3) p-value
- 귀무가설이 참일 때 현재 관측된 데이터보다 극단적인 결과가 나올 확률  
- p < 유의수준 → 귀무가설 기각
  
- 계산 방법
  1. 검정 통계량을 계산한 뒤에 귀무가설 하에 해당 통계량이 나올 확률을 누적해서 계산
  2. 유의 수준보다 p값이 낮으면 귀무가설을 기각, 아닐 경우 기각할 수 없음
 
  Q. p값이 유의수준보다 높으면 귀무가설을 체택한다 ?
  A. p 값이 높으면 대립가설을 체택하지 못하는 것이지 귀무가설을 체택하는 것이 아님

  Q. p값이 0.03이면 귀무가설이 틀릴 확률이 3%이다?
  A. p값은 귀무가설이 참일 때 이런 데이터가 나올 확률이지 귀무가설이 틀릴 확률이 아님
  
  -> p값은 애초에 귀무가설이 참인 가정하에 계산한 조건부 확률
  
<img width="509" height="92" alt="image" src="https://github.com/user-attachments/assets/cf02236e-b0ef-4c17-aaee-c81ab22e24bf" />

---

## 4. A/B Test
- 통제군과 처치군의 반응 차이를 비교해 처치 효과를 추정하는 실험 설계 방법.

### 1) ATE (평균 처리 효과)
- A/B test가 추정하고자 하는 효과로 처치군의 반응에서 통제군의 반응을 뺀 값의 예상값

<img width="253" height="38" alt="image" src="https://github.com/user-attachments/assets/94965525-2e12-4319-8e09-227004d9596d" />

### 2) 가설검정
- t통계량을 사용하며 가설은 아래와 같음
- 
H0 : 처리효과 없음 = 통제군과 처치군의 모평균이 같다.

H1 : 처리효과 있음 = 통제군과 처치군의 모평균이 같지 않다.

<img width="285" height="153" alt="image" src="https://github.com/user-attachments/assets/6fe68472-b90a-4ecf-8a43-868ea45ddaaa" />

### 3) 가정
-  SUTVA : 다른 참가자의 처리 상태가 개별 응답에 영향을 주지 않음
- 랜덤 배정 : 모든 혼란 요인이 반영되지 않도록 통제군과 처치군 선정은 랜덤으로 배정됨
- 독립성 : 각 실험 단위는 독립적으로 반응함
- 표본 크기 충분성 : 통계적 검정력을 확보할 수 있는 충분한 크기 필요
- 단일 처리 요인 : 동시에 하나의 요인만 조작되어야 인과 추론이 명확함

---

## 5. MLE와 MAP
- **MLE** : 관측 데이터의 가능성을 최대로 하는 파라미터 추정 (사전정보 사용 X)  
- **MAP** : 관측 데이터 + 사전정보 함께 고려  

### 1) MLE 
-  관측된 데이터가 가장 가능성이 높게 발생하도록 하는 모델 파라미터를 찾는 방법

<img width="438" height="83" alt="image" src="https://github.com/user-attachments/assets/2e58f815-2cb4-4817-bf29-3293380e0bfb" />

-> 로그변환 : 미분과 최적화를 더 쉽게 하기 위해 로그를 취함

> MLE는 사전정보를 사용하지 않고 오직 관측된 데이터에 기반하여 계산됨

하지만 사전정보가 존재하는 상황에서 이를 굳이 무시할 필요가 없다면 ?

### 2) MAP
- 관측된 데이터와 사전 정보를 함께 고려해서 가장 가능성이 높은 모델 파라미터를 찾는 방법
- 기본 형태 : 베이즈 정리를 통해 유도됨

<img width="891" height="175" alt="image" src="https://github.com/user-attachments/assets/b25c2756-e642-4007-8c3e-a9652d39971d" />

-> 로그변환 : 미분과 최적화를 더 쉽게 하기 위해 로그를 취함

<img width="483" height="65" alt="image" src="https://github.com/user-attachments/assets/444e138c-fa32-4a26-8513-9d2cee947ef5" />

* MAP는 관측된 데이터 뿐만 아니라 사전 정보도 같이 고려함

<img width="365" height="84" alt="image" src="https://github.com/user-attachments/assets/cf4701b2-bfcc-4d26-b642-fe8d7d54c428" />

* 딥러닝에서의 목표는 아래의 파라미터를 찾는것

<img width="371" height="87" alt="image" src="https://github.com/user-attachments/assets/4748dc81-deca-4d6f-8e05-e708c78ff73e" />

- 여기서 Loss자리에 넣는 손실 함수가 MLE와 MAP에 기반함
- MLE와 MAP는 최대화를 하는 파라미턴인데 Loss는 최소화의 대상이므로 마이너스 부호를 취해서 사용 → MLE와 MAP가 최대화되면 Loss함수는 최소화됨
- 아래의 식에 적절한 확률 분포를 대입하여서 손실함수로 사용

<img width="347" height="103" alt="image" src="https://github.com/user-attachments/assets/559f6ed4-7c84-401a-82d3-8a26fe8bfece" />

1. MSE : MLE 기반에 정규 분포를 대입
   <img width="240" height="81" alt="image" src="https://github.com/user-attachments/assets/87755b60-7451-4ed9-9362-a731cff7a050" />

2. Binary Cross-Entropy : MLE 기반에 베르누이 분포를 대입
<img width="359" height="55" alt="image" src="https://github.com/user-attachments/assets/b94cffe3-c55f-4e2d-97a3-f61620437300" />

3. MAP 기반 손실함수 : prior분포 가정에 따라서 L2 or L1의 정규화 항을 가짐
   - 정규 분포를 가정 : L2정규화
     <img width="414" height="48" alt="image" src="https://github.com/user-attachments/assets/241a4120-5b97-4a73-97ec-e2dc539ebe62" />

   - 라플라스 분포를 가정 : L1정규화
     <img width="394" height="54" alt="image" src="https://github.com/user-attachments/assets/159be9f0-07cd-4451-b0d7-34358fa42545" />

# 03. 회귀분석 기초

## 1. 단순회귀분석 (SLR)
- Simple Linear Regression Model : 독립변수가 종속변수에 대해서 선형적인 영향을 미친다는 가정하에 세워진 모델 → b0은 절편 , b1은 기울기, u는 오차항
<img width="351" height="70" alt="image" src="https://github.com/user-attachments/assets/d1c7e53d-908c-42a9-bd35-30c996eea10f" />

## 2. OLS 회귀모형
- 오차의 제곱합을 최소로 만드는 것을 기준으로 계수를 결정하는 모델
<img width="270" height="70" alt="image" src="https://github.com/user-attachments/assets/5b0efa03-9be3-47be-9716-eebd141a9622" />

## 3. OLS가 BLUE이기 위한 조건
- BLUE : 가장 이상적인 OLS 추정값으로 해당 가정들에 대한 이해를 통해 선형 회귀에 대한 추가적 이해 가능
<img width="918" height="252" alt="image" src="https://github.com/user-attachments/assets/0e27b5c7-a460-4d2a-9f85-5cf5aec05a9f" />

### 1) 선형성
- SLR 모델과 같이 모집단에서도 회귀계수에 대한 선형성을 나타낼 것이다 라는 가정
-> 선형성 가정은 β의 선형 결합을 뜻함
- 독립변수와 종속변수 간의 선형성을 이미하는 것이 아님
-> x와 y가 직선 관계라는 뜻은 아님
- 관련 값을 계산하는 과정에서 해당 조건이 없으면 값 도출이 불가능함
-> β에 대해 선형이지 않으면 OLS 계산 자체가 불가능
<img width="523" height="166" alt="image" src="https://github.com/user-attachments/assets/824725dc-df11-4e8f-a1aa-dde0ca078308" />

### 2) 랜덤 추출 = i.i.d

- 관측치가 서로 독립적이고 동일한 분포에서 왔다는 가정
- 앞서 살펴본 중심극한정리를 사용하기 위한 조건

### 3) 표본변수 내 설명변수 값 다름

- 설명변수 (x) 의 값이 동일한 상수가 아니라는 점
- x의 값이 동일하면 기울기를 추정할 수 없음

### 4) 평균 독립

- 오차의 평균 값이 어떤 x에서든 0이라는 가정
- 정확히는 0일 필요는 없고, 상수면 가능
- OLS 추정량이 불편하기 위한 가정

<img width="226" height="44" alt="image" src="https://github.com/user-attachments/assets/2654425e-f014-4776-87d9-ecd396aa6602" />

### 5) 동분산

- 오차항의 분산이 시그마 제곱(상수)로 고정되어서 x에 의존하지 않는다는 가정
- 잔차를 최소화하는 값을 도축하기 위해서는 각 x에서의 분산이 동일하다는 가정이 필요
- 동분산 가정이 위배되어도 불편 추정량을 구할 수는 있으나 BLUE는 아님

  
## 4. 모델 성능 평가지표

### 1. MSE
<img width="290" height="100" alt="image" src="https://github.com/user-attachments/assets/f6795e64-2361-43f5-865c-6652e1ace8d4" />

- 오차를 제곱해 평균낸 값
- 제곱을 하기에 더 큰 오차에 대해 더 큰 패널티 부여
- PMSE는 MSE에 루트를 씌운 것으로 단위가 기존 y와 같아져서 직관적 해석이 가능하다는 장점이 있음

### 2. MAE
<img width="312" height="86" alt="image" src="https://github.com/user-attachments/assets/b0adc36a-a409-4835-ab10-2e79abc6a54b" />

- 오차에 절댓값을 부여한 값
- 이상치를 덜 강조하고 싶은 경우에 사용됨

### 3. 회귀 문제에서의 평가 지표
-  R^2 : 전체 종속변수의 변동 (SST) 중에서 회귀모형이 설명한 비율 (설명된 변동 / 전체 변동)을 의미
  
  <img width="302" height="99" alt="image" src="https://github.com/user-attachments/assets/2095d9bc-9a18-4564-b0af-053cd93087bf" />

<img width="679" height="118" alt="image" src="https://github.com/user-attachments/assets/b61eb432-d967-4d22-819e-b36db16d72b1" />
