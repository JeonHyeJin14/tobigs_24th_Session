# 01. 지도학습 vs 비지도학습

<img width="1085" height="453" alt="image" src="https://github.com/user-attachments/assets/d62a3f7b-e140-49a7-b42d-4e76029d3cc7" />

# 02. 지도학습
: 입력 데이터와 정답 (lable)이 쌍으로 주어진 데이터를 통해 학습하는 방식
- 목표 : 학습 통해 처음보는 데이터의 정답을 예측하는 모델을 만드는 것
- 대표적 모델 : 선형 회귀, 로지스틱 회귀, 결정트리, SVM 등등

## 1. 회귀와 분류문제

### 1) 회귀
- 연속적인 수치를 예측
- 지표 : MSE, RMSE 등등

### 2) 분류
- 정해진 카테고리를 예측하는 문제
- 지표 : Accuracy, Precision 등등

## 2. 회귀 문제의 손실함수 
- 모델의 예측값이 실제값과 얼마나 다른지를 수치로 측정
<img width="540" height="260" alt="image" src="https://github.com/user-attachments/assets/c154d8cb-ed1c-4bb9-8e72-f1851d3eace5" />

<img width="505" height="254" alt="image" src="https://github.com/user-attachments/assets/ce865096-9fd4-43ef-a4ac-f683339ef137" />

## 3. 분류문제의 손실함수
- 모델이 얼마나 정확하게 분류했는가를 평가

1. Accuracy (정확도)
- 전체 중에 맞춘 비율

2. Pecision (정밀도)
- 참이라고 예측한 것 중 실제로 참인 비율

3. Recall (재현율)
- 실제로 참인 것 중에 잘 맞춘 비윤

<img width="588" height="259" alt="image" src="https://github.com/user-attachments/assets/8da37fa6-396c-4aea-9979-ca2a79f8f8cf" />

## 4. 선형 회귀 (Linear Regression)
- 데이터를 직선으로 근사해 예측하는 대표적 지도학습 알고리즘
<img width="315" height="79" alt="image" src="https://github.com/user-attachments/assets/128b83fd-aa58-489e-8e8f-97cd339a1307" />

<img width="369" height="255" alt="image" src="https://github.com/user-attachments/assets/61568c99-d17f-4917-bae5-ff24767c885d" />

- 가중치 : feature가 target 예측에 미치는 영향정도를 나타내는 값
- 편향 (bias): 모든 feature가 0일 때 target 값이며 항상 일정한 방향으로 치우친 예측 방지

## 5. 과적합과 규제 (Regularization

### 과적합 (Ocerfitting) : 훈련 데이터에 완벽하게 적합 -> 테스트에서는 성능 저하 = 일반화 x

### 규제 (Regularization) : 모델의 복잡도를 줄이는 방법, 과적합 대표적인 해결방안 

- 규제항을 추가해 의도적으로 손실함수를 키워서 가중치 크기를 너무 크지 않도록 제어

<img width="447" height="135" alt="image" src="https://github.com/user-attachments/assets/f1ad478a-1beb-4795-9c18-17e9985fdd52" />

- 모든 가중치를 조금씩 작게 만듦
- 가중치가 클수록 더 큰 패널티를 받아 점점 0으로 수렴하지만 0이 되지는 않는다.

<img width="468" height="128" alt="image" src="https://github.com/user-attachments/assets/01855d22-59c4-479e-b889-678a8dc8571c" />

- 일부 가중치를 정확히 0으로 만듦
- 특성 선택 효과가 있어서 불필요한 특성을 자동으로 제거하며 해석이 쉽다

## 6. 로지스틱 회귀 (Logistic Regression)

- 이진 분류(Binart Classification)을 수행하는 선형 모델
- 선형 회귀로 계산된 값 -> 시그모이드 함수를 통해 확률값으로 변환
<img width="531" height="100" alt="image" src="https://github.com/user-attachments/assets/673d53a0-51d0-4456-aa52-bcdca5f86917" />

- 예측값은 0~1사이의 확률, 기준값을 넘으면 클래스 1 아니면 0
- 손실함수는 Cross Entropy 사용

## 7. Decision Tree (의사 결정 트리)
- 입력 데이터의 특성을 기준으로 조건 분기를 반복하면서 여러 그룹으로 나누는 분류, 회귀 모델

### 1. 배경  : 선형 모델을 복잡한 데이터 경계를 표현하기가 어려움 -> 비선형으로 데이터 분리 가능

### 2. 동작 방식
1. 모든 feature 중 데이터를 가장 잘 나눌 수 있는 기준 선택
2. 해당 기준으로 데이터 분할 -> 이 과정을 반복함
3. 정지 조건을 만족하면 분할 중단하고 리프 노드 설정

### 3. 분할 기준
- 분류 문제 : 지니 불순도, 엔트로피 -> 값이 작을수록 그룹이 잘 나뉜 것
- 회귀 문제 : 분산, MSE 사용

### 4. 과적합 문제
- 트리를 너무 깊게 만들면 과적합 발생
- 해결 방법 : 최대 깊이 제한 , 최소 샘플 수 제한 등

### 5. 활용 및 확장
- 앙상블 모델로 발전해 자주 사용됨
  - Random Forest, Gradient Boosting 등

## 8. 결정트리의 분할 기준 : Gini Index와 Entropy

### 1) 공통 개념
- 목적 : 분할 전후의 불순도를 계산해 불순도가 가장 크게 감소하는 분할 선택
- 불순도 : 데이터 집합이 서로 다른 클래스가 얼마나 섞여있는지 나타냄

### 2) Gini INdex 
- 데이터를 무작위로 선택했을 때 잘못 분류될 확률
- 클래스가 하나로 완벽하게 분리된 경우 =0
- 값이 클수록 클래스가 섞여있음
- <img width="234" height="54" alt="image" src="https://github.com/user-attachments/assets/b448552a-b898-4360-9b05-1ac202ac82be" />

### 3) Entropy
- 데이터의 혼잡도 측정 (불확실성)
- 클래스 분포가 균등함 -> 값이 최대
- 하나의 클래스만 존재 -> Entropy = 0
- <img width="356" height="67" alt="image" src="https://github.com/user-attachments/assets/3bfe701d-024f-49c7-9814-710577c2021a" />
- 값이 클수록 클래스가 균등하게 섞임

## 9. SVM (Support Vector Machine)

<img width="190" height="157" alt="image" src="https://github.com/user-attachments/assets/99cfd6d6-ed56-4394-ab5e-af7debde2ee6" />

### 1) 등장 배경
- 분류 경계가 불명확할 경우 로지스틱 회귀는 불안정
- SVM은 이런 사오항에서 클래스 간 경계를 명확히 구분하는 초 평면을 찾음

### 2) 목표
- 최대 마진을 만드는 경계를 찾는 것
- 경계 결정은 Support Vector (초평면에 가장 가까운 데이터 포인트)가 수행

### 3) 기본 원리
1. 결정 경계 : SVM은 데이터를 가장 잘 분류하는 최적의 초평면을 찾음
2. 마진 최대화 : 초평면과 각 클래스 데이터 간 거리를 최대화하여 일반화 성능을 높임
3. 서포트벡터 : 마진과 경계를 결정하는 핵심 역할

### 4) 핵심 포인트
- SVM은 클래스 간 경계를 가장 잘 구분짓는 직선 혹은 초평면을 찾는 분류 알고리즘
- 고차원 데이터에도 적용 가능 ( 비선형 분류 시 커널 기법 사용 )

## 10. Hard Margin과 Soft Margin

<img width="261" height="146" alt="image" src="https://github.com/user-attachments/assets/7aae17b5-0b30-4527-b87c-62a4b823ba91" />

### 1) Hard Margin
- Outlier를 무시하지 않고, Support Vector를 찾아 완벽한 분리 경계를 만드는 방식
- 마진이 매우 좁아질 수 있고, 과적합 가능성이 높음 (노이즈와 이상치에 민감함)
- Outlier가 경정 경계를 왜곡 시킬 위험 존재

### 2) Soft Margin
- Hard Margin의 한계 : 모든 데이터를 초평면으로 완벽히 분리하려 하면 이상치로 인해 경계가 극단적으로 학습됨 -> 과적합 발생
- - 일부 오차를 허용 (슬랙 변수 도입) 하면서 전체적 마진을 최대화
  - 장점 : 노이즈나 이사잋에 더 강건하고 일반화 성능이 향상됨
  - 하이퍼파라미터 : 오차 허용이 증가하면 마진이 넓어짐 (경계가 유연)
 
## 11. 커널 트릭 (Kernel Trick)
<img width="376" height="148" alt="image" src="https://github.com/user-attachments/assets/89bc3c3b-ae07-420d-bed1-8c9916eeb0e5" />

### 1) 정의
- 입력 데이터를 고차원 공간으로 매빙하여 고차원에서는 선형 분리가 가능하도록 만드는 기법
- SVM 등 비선형 문제 해결에 활용

### 2) 등장 배경
- 기존 SVM 한계 : 현실 데이터는 대부분 2차원 평면에서 선형 분리 불가
- 고차원 공간에서는 데이터가 더 잘 분리될 수 있음

### 3) 핵심 아이디어
- 데이터를 실제로 고차원 공간으로 옮기는 것이 아니라 커널 함수를 이용해 내적 연산만으로 고차우너에서 계산한 것과 같은 결과를 얻음
- 연산 효율성을 유지하면서도 고차원 변환의 장점을 활용가능

### 4) 주요 거널 함수
- RBF
- Linear Kernel
- Polynomial Kernel

### 5) 효과
- 비선형 데이터도 선형 분리 가능하게 함
- 차원의 저주 문제를 완화하면서 분류 성능 향상

## 12. SVR (Suppport Vector Regression)
<img width="297" height="127" alt="image" src="https://github.com/user-attachments/assets/8d0b807a-175f-4bfc-9033-6c2c865d208e" />

### 1) 개념
- SVM을 분류가 아닌 회귀 문제에 적용한 모델
- 목표 : 마진 안쪽에 최대한 많은 점을 포함시키는 회귀선 또는 초평면을 찾음

### 2) 아이디어
- 마진 안쪽 -> 예측 오차 허용 ( 패널티 없음 )
- 마진 밖 -> 예측 오차에 패널티부여
- 결과적으로 , 불필요하게 모든 점에 완벽히 맞추지 않고 중요한 패턴만 학습

### 3) 장점
- 기존 선형 회귀보다 모델이 덜 민감하고 일반화 성능이 우수함
- 이상치에 덜 영향 받음

### 4) 시각적 이해
- SVM 분류 : 마진 밖의 점에만 영향을 받음
- SVR 회귀 : 마진 밖의 예측 오차에만 패널티 부여

# 03. 비지도학습

## 1. 비지도 학습
<img width="231" height="95" alt="image" src="https://github.com/user-attachments/assets/232ccebf-aa4b-47f6-9ea2-351af3e6454f" />

### 1) 등장배경
- 지도학습의 한계 : 실제 데이터 중 Label이 없는 경우가 훨씬 많음
- 라벨링 작업 : 비용 및 시간이 많이 소요됨

### 2) 정의
- 정답 (Label)없이 주어진 데이터에서 새로운 규칙성을 찾는 학습 방식
- 데이터의 패턴, 구조, 군집, 이상치등을 스스로 파악함

### 3) 대표 알고리즘
1. K-Means
2. Hierarchical Clustering
3. DBSCAN
4. GMM (Gaussian Mixture Model)

## 2. 군집화

### 1) 정의 
- 정답 없이 유사한 데이터끼리 그룹화 하는 비지도 학습 기법
- 거리, 밀도, 분포, 통계적 유사성 등을 기준으로 군집 생성

### 2) 목표
- 군집 내부 -> 유사한 데이터끼리 묶임
- 군집 간 -> 서로 다른 데이터로 구분

### 3) 활용사례
- 이상치 탐지
- 고객 세분화

### 4) 분류와의 차이
- 분류 : 미리 정의된 그룹에 데이터를 넣음
- 군집화 : 데이터를 보고 그룹을 스스로 형성

### 5) 대표 알고리즘 비교
<img width="1178" height="384" alt="image" src="https://github.com/user-attachments/assets/b00906e0-abdc-4261-9f53-2dbfb84eed1d" />

## 3. K-Means Clustering
<img width="442" height="204" alt="image" src="https://github.com/user-attachments/assets/af19a2f1-7de0-4d57-aecf-d89720f01456" />

### 1) 정의
- 군집 수 k를 미리 정하고, 각 군집의 중심을 기준으로 데이터를 그룹화하는 알고리즘

### 2) 특징
- 직관적이고 속도가 빠름
- 군집의 모양이 둥글고 균일할 때 성능이 좋음
- 이상치에 민감
- 초기 centroid 값에 민감 -> 개선 방법 : K-Means ++

### 3) 동작 과정
<img width="824" height="395" alt="image" src="https://github.com/user-attachments/assets/0880b1a4-fcb6-45f9-b773-65f70fa2edbb" />

1. K개의 중심점을 초기에 설정
2. 각 데이터 포인트를 가장 가까운 중심점에 할당
3. 각 군집의 중심점을 재계산
4. 중심점 변화가 없을 때 까지 반복

## 5. hierarchical Clustering (계층적 군집화)
<img width="438" height="247" alt="image" src="https://github.com/user-attachments/assets/a6acc633-e59e-424d-804e-a470c036ed0f" />

### 1) 개념 
- 트리구조를 이용해 데이터를 유사도에 따라 계층적으로 묶어가는 군집화 방식
- Agglomerative 방식 : 개별 데이터에서 점차 합침
- Division 방식 : 전체 데이터에서 점차 분리

### 2) 동작 과정
<img width="463" height="257" alt="image" src="https://github.com/user-attachments/assets/60e0bb6d-8839-42db-87dc-c7fc14dec50d" />

1. 가장 가까운 거리에 있는 데이터를 서로 묶음
2. 묶인 군집 간의 거리를 계산해 반복적으로 결합
3. 최종적으로 하나의 군집이 될 때까지 진행
4. 결과를 Dendrogram으로 표현, 자르는 높이에 따라 군집 수 조절 가능

### 3) Cluster Linkage (군집 간 거리 계산 방법)
- Single Linkage : 두 군집 사이의 가장 가까운 점 거리
- omplete Linkage : 두 군집 사이의 가장 먼 점 거리
- Average Linkage : 두 군집 간 모든 점 거리 평균

### 4) 특징 
- 군집의 개수를 사전에 지정하지 않아도 됨
- 시각화로 군집 구조 해석 용이
- 대규모 데이터에서는 계산비용이 큼

## 6. DBSCAN

### 1) 개념
- 데이터의 밀도를 기준으로 군집을 형성
- 밀도가 낮은 점은 노이즈로 간주
- 군집 수를 사전에 지정할 필요 없음

### 2) 주요 하이퍼파라미터
- Epsilon : 한 점 주변을 탐색할 반경
- MinPts : 군집을 형성하는 데 필요한 최소 데이터 포인트 수 (밀도 기준)

### 3) 특징
1. 장점
   - 군집 수 지정 불필요
   - 복잡한 형태의 군집 탐지 가능
   - 이상치 탐지 가능
2. 단점
   - epsilon과 MinPts 설정에 민감함
   - 파라미터에 따라 결과 해석이 어려움

### 4) DBSCAN 수행 단계
<img width="407" height="312" alt="image" src="https://github.com/user-attachments/assets/adbf2b00-69ab-4ccb-9fdd-28a5e71dec98" />

1. 임의의 데이터 포인트를 선택
2. 선택한 포인트의 반지름 Epsilon 내 이웃 점 탐색
3. 이웃 점이 MinPts 이상이면 새로운 군집 생성
4. 새로 포함된 포인트 중 core point 조건을 만족하는 점이 있으면 연결된 점들을 계속 추가
5. 모든 포인트에 대해 반복
6. 어느 군집에도 속하지 못한 점은 노이즈로 분류

## 7. GMM (Gaussian Mixture Model)
<img width="403" height="239" alt="image" src="https://github.com/user-attachments/assets/7530d9f4-3a66-4ba3-a819-bb662edeb501" />

### 1) 개념
- 데이터가 여러 개의 정규분포 (가우시안 분포)를 따른다고 가정
- 각 데이터가 군집에 속할 확률을 계산하는 확률적 군집화 알고리즘

### 2) 특징
- 각 군집은 하나의 가우시안 분포로 모델링
- 하나의 데이터 포인트는 여러 군집에 속할 수 있으며, 각 군집에 속할 확률을 가짐
- ex) 고객 A → 70% 확률로 군집1, 30% 확률로 군집2에 속함

### 3) 학습 방식 : EM 알고리즘
1. E-step : 각 데이터가 각 군집에 속할 확률 계산
2. M-step : 계산된 확률을 기반으로 가우시안 파라미터 (평균 , 분산 ) 갱신
3. 수렴할 때까지 반복

### 4) 장점과 단점
<img width="812" height="296" alt="image" src="https://github.com/user-attachments/assets/563aecd5-14f2-4367-b992-f022b1e74658" />

## 8. 차원축소

### 1) 개념
- 고차원 데이터를 더 적은 수의 중요한 특성 (차원)으로 압축하는 기법
- 목적 : 분석 및 시각화 용이성 향상, 계산 효율 개선, 노이즈 제거

### 2) 필요성
- 차원이 높을수록 분석과 시각화가 어려움 -> 차원의 저주 발생
- 차원 축소로 노이즈 제거, 시각화, 계산 효율 향상 가능

### 3) 대표 활용 예ㅣ
- 이미지 압축
- 데이터 전처리
- 고차원 데이터 시각화 (2D, 3D)

### 4) 대표 기법 = PCA (주성분분석)
- 데이터 분사닝 큰 방향으로 투영해 정보 손실 최소화
- 차원을 축소하면서 주요 패턴 보존
- 주로 2D, 3D 시각화와 전처리에 활용

### 5) 장점 및 단점
<img width="822" height="250" alt="image" src="https://github.com/user-attachments/assets/17232289-5ba9-4d32-9a6b-0d9e9df91e17" />


