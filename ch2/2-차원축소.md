# 01. 차원과 차원축소

## 1. 차원
- 공간 내에 있는 점의 위치를 나타내기 위한 필요한 축의 개수
- 공간 내에 있는 점 = 데이터
- 필요한 축 - Feature
-> 차원의 크기가 커질수록 데이터를 해석할 수 있는 정보를 더 많이 가지고 있음을 의미함

## 2. 차원의저주
- 데이터 차원 증가
-> 데이터를 담고 있는 공간의 부피 기하급수적으로 증가
-> 데이터 밀도가 희소해짐 = 데이터 간 거리 증가
<img width="423" height="151" alt="image" src="https://github.com/user-attachments/assets/3cdc5ae9-165d-4213-98aa-24c442941ec7" />

<img width="694" height="251" alt="image" src="https://github.com/user-attachments/assets/ed04f185-35ae-44e6-82cb-fbd7db6a998b" />

# 02. EVD / SVD

## 1. 고유값과 고유 벡터
<img width="251" height="80" alt="image" src="https://github.com/user-attachments/assets/48fcb686-67cf-4f4f-bbb3-d0ed39f9909c" />

- 고유벡터 : 선형 변환을 적용해도 방향이 바뀌지 않는 벡터
- 고유값 : 고유벡터의 방향으로 나타난 크기 변화 비율

<img width="284" height="245" alt="image" src="https://github.com/user-attachments/assets/8bc16780-29f5-4fd7-9c3a-d779b42c4b18" />

## 2. EVD
<img width="187" height="47" alt="image" src="https://github.com/user-attachments/assets/78f604c8-d9f6-4fc8-8015-4452cce809ca" />

- V : 고유벡터를 열벡터로 가진 행렬
- A : 고유값으로 이루어진 행렬
- A의 조건
  1. n * n 정방행렬
  2. n개의 선형 독립 고유 벡터 = 대각화 가능
    
- 고유벡터 기준으로 좌표계를 돌리고 -> 고유값만큼 늘리고 -> 다시 원래 좌표계로 돌린다
- 복잡한 선형 변환도 고유벡터라는 기준으로 보면 단순히 스케일 조정만 한 것 처럼 해석 가능
- ex)
- <img width="373" height="220" alt="image" src="https://github.com/user-attachments/assets/918196ce-5c3f-4e8f-afda-f003111a9ae9" />

## 3. SVD
<img width="661" height="272" alt="image" src="https://github.com/user-attachments/assets/e29ba352-3fdc-4464-8a25-2f944c3b6646" />

- 기하학적 의미

<img width="310" height="247" alt="image" src="https://github.com/user-attachments/assets/2bac04cc-b91e-43fd-89f9-9da6e337d10e" />

# 03. PCA / LDA

## 1) 주성분분석 (PCA) 
- 가장 인기있는 차원축소 알고리즘
<img width="691" height="275" alt="image" src="https://github.com/user-attachments/assets/dcb5cb24-a97d-4b60-8ea1-a0d275d938bd" />

- 분산을 최대로 보존할 수 있는 축을 선택하는 이유?
- 분산이 큼 = 데이터 사이들의 차이점이 명확함 = 데이터 안에 담긴 변화량이 많다 = 정보 손실 최소화 = 좋은 모

<img width="568" height="214" alt="image" src="https://github.com/user-attachments/assets/651060b5-c162-45c3-bc46-7c2c49f2803d" />

- 과정
1. 가장 큰 분산을 가지는 첫 번째 축 생성
2. 첫 번째 축에 직각이 되는 벡터 선택
3. 두 번째 축과 직각이 되는 벡터 선택
4. 원하는 차원 수만큼 벡터 축 생성 반복

- 직각이 되는 벡터를 선택하는 이유는?
<img width="224" height="210" alt="image" src="https://github.com/user-attachments/assets/70083f26-203c-4044-b68b-0f10bba6fd20" />

- 중복된 정보없이 독립적인 방향으로 데이터를 압축하기 위해서 = SVD
- 독립적인 방향 : 한 축의 값이 다른 축의 값과 전혀 상관없음 = 상관계수 0 = 새로운 축마다 서로 다른 정보만 담김
<img width="412" height="127" alt="image" src="https://github.com/user-attachments/assets/2b12825b-2c74-4cba-9cc0-623484fe6048" />

## 2) 선형 판별 분석법 (LDA)
<img width="218" height="201" alt="image" src="https://github.com/user-attachments/assets/3bb9cf29-f468-4fd8-9d51-e48b7db289c4" />

- 클래스를 잘 구분하는 축을 찾는게 목표
- 클래스 내의 분산은 최소, 클래스 간의 거리는 최대가 되게 하는 벡터를 찾아 데이터를 투영

## 3) PCA VS LDA
<img width="734" height="278" alt="image" src="https://github.com/user-attachments/assets/c925ad72-f740-4480-b76e-9436cec57ff9" />

# 04. t-SNE / UMAP

## 1. t_SNE
- 고차원 데이터를 저차원 공간으로 시각화하기 위해 사용되는 비선형 차원 축소 기법
<img width="628" height="162" alt="image" src="https://github.com/user-attachments/assets/ad8ada60-56f1-4642-a0d5-e29d02c190ea" />

1. 고차원 데이터에서의 유사성 계산
-> 데이터 포인트 i가 데이터 포인트 j를 선택할 확률 (가우시안 분포 기반) -> 데이터 포인트 i와 j가 얼마나 유사한지 확률로 표현
   
<img width="395" height="75" alt="image" src="https://github.com/user-attachments/assets/1a65f311-0207-4c9b-85d2-069fd5c766f5" />

2. 저차원 데이터에서의 유사성 계산 = 친밀도 계산
-> 데이터 포인트 i가 데이터 포인트 j를 선택할 확률 (t분포 기반)
   
<img width="373" height="100" alt="image" src="https://github.com/user-attachments/assets/84e11445-4b1f-4bdc-92bf-23a9f1dda840" />

<img width="689" height="241" alt="image" src="https://github.com/user-attachments/assets/a5ccabcd-cb44-4dc7-90f5-762d3ff0d7f9" />

<img width="765" height="333" alt="image" src="https://github.com/user-attachments/assets/6c88329e-ae71-4742-8514-36819308196c" />

3. KL Divergence 최소화
- KL Divergence = “두 확률 분포가 얼마나 다른지”를 재는 거리 척도
- 고차원의 친밀도와 저차원의 친밀도가 똑같아지도록 KL Divergence를 최소화하는 방향으로 점들의 위치를 조금씩 움직

<img width="399" height="82" alt="image" src="https://github.com/user-attachments/assets/971f2608-2189-49f8-8dc4-b6d0040b9cad" />

- 고차원 공간과 저차원 공간의 유사성 확률 분포 사이의 차이공식 -> 이것을 최소화 하는 것이 t-SNE 목표
- 비슷한 데이터끼리는 뭉침
- 다른 데이터끼리는 멀어짐
- 복잡한 데이터 구조를 2D/3D에서도 잘 볼 수 있게 됨

4. 확률적 경사 하강법 (SGD) 최적화
- 전체 데이터 대신 일부 점들을 뽑아서 그때그떄 KL DiVergence가 줄어드는 방향으로 좌표를 조금씩 이동시킴

<img width="370" height="100" alt="image" src="https://github.com/user-attachments/assets/a877943d-2b76-4604-ba81-7c50495c8635" />

- 확률적 경사 하강법으로 저차원 좌표 반복 최적화
- KL Divergence를 최소화하는 방향으로 갱신

### 장점 및 단점
- 장점
  1. 비선형 변환 가능 -> PCA와 달리 복잡한 구조 시각화 가능
  2. 겹침 완화 -> 데이터 간 분리 강조
  3. 하이퍼파라미터 영향 적음, 이상치에 둔감
  4. 클러스터 구조 표현에 강점

- 단점
  1. 연산량이 많음
  2. 초고차원 데이터 바로 축소 어려움 -> 보통 50차원 이내로 사전 축소 후 적용
  3. 결과 비결정적 -> 매 실행 시 시각화 결과 다름
  4. 정보 손실 가능성 -> 데이터 왜곡 우려가 있음

  ## 2) UMAP
  <img width="991" height="320" alt="image" src="https://github.com/user-attachments/assets/d347f50a-cf2c-4c4a-b719-6f29c574215b" />

- 파라미터
  1. n_neighbors : 초기 고차원 그래프 생성 시 사용되는 nearest neighbor의 숫자로 가장 중요한 파라미터
  2. min_dist : 저차원 공간에서 포인트 간의 최소거리 , UMAP이 포인트들을 얼마나 촘촘하게 묶을지 조절

<img width="983" height="317" alt="image" src="https://github.com/user-attachments/assets/62e6da91-1437-4364-8d2e-47c86230d9dd" />

<img width="923" height="324" alt="image" src="https://github.com/user-attachments/assets/36878c3b-518a-4e57-8b40-53e9a473422d" />

### 장점 및 단점
- 장점
  1. t_SNE보다 훨씬 빠름
  2. 결과 재현성이 높아서 반복 실험이 필요한 대규모 데이터에 효과적이다

- 단점
  1. 하이퍼파라미터가 결과에 큰 영향을 줌 -> 다양한 파라미터 조합 실험이 필요
  2. 클러스터의 상대적 크기는 의미가 없음 -> 클러스터가 크게 보인다고해서 중요하거나 크다는 의미가 아님
  3. 클러스터 간 거리도 의미가 없을 수 있음 -. 전역 구조는 보존되지만, 클러스터간 거리는 해석에 주의해야 함
  4. 랜덤 노이즈가 가짜 클러스터 처럼 보일 수 있음 -> 특히 n_neighbors가 작을 때는 더 주의해야함
  5. 하나의 시각화만으로 판단하지 않는 것이 좋음 -> UMAP은 확률적 알고리즘이므로 여러 파라미터로 다양함
