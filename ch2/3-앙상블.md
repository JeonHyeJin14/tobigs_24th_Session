# 01. Ensemble 

## 1. 앙상블
- 여러 개의 모델을 결합해 하나의 모델보다 더 좋은 성능을 내도록 하는 기법
<img width="622" height="290" alt="image" src="https://github.com/user-attachments/assets/29b8381d-067f-4323-afb1-9db1a0a0d75b" />

## 2. No Free Lunch 이론
- 모든 문제에서 항상 성능이 좋은 단일 알고리즘은 존재하지 않는다는 정리
- 특정 데이터에서만 잘 작동하는 단일 모델보다는 다양한 모델의 강점을 결합해서 여러 상황에서도 안정적으로 좋은 성능을 낼 수 있는 새로운 모델 개발

## 3. Diversity 이론
- 서로 다른 관점을 가진 모델들을 좋바할수록 전체 모델의 성능이 높아질 수 있다는 정리
- 각 모델이 서로 다른 방식으로 데이터를 해석하고 예측한다면, 훨씬 더 강력한 예측력을 가질 수 있음

## 4. Bias-Variance Trade-off
### 이상적인 경우 (Low Bias, Low Variance)
<img width="349" height="315" alt="image" src="https://github.com/user-attachments/assets/f4b2acf3-5fc6-4744-aaae-8cd671c1ef2d" />

- 예측이 정답 주변에 모여있음 = 오차가 작고 안정적임
- 데이터의 복잡한 패턴을 잘 포착하면서, 불필요한 노이즈에 과하게 반응하지 않음
- 학습과 일반화가 균형을 이룸

### 1) 편향이 큰 경우 (High Bias, Low Variance)
- 예측이 한 쪽으로 일관되게 빗나감
- 모델이 너무 단순 -> 과소적합상태

### 2) 분산이 큰 경우 (Low Bias, High Variance)
- 예측이 정답 근처지만 산발적으로 흩어짐
- 모델이 너무 복잡함 -> 과적합 상태

<img width="356" height="336" alt="image" src="https://github.com/user-attachments/assets/2a927ea7-7c67-4597-9c34-e2b30556db2d" />

<img width="646" height="326" alt="image" src="https://github.com/user-attachments/assets/572ffe0a-2459-40d1-ab08-18fda06b7add" />

<img width="499" height="158" alt="image" src="https://github.com/user-attachments/assets/48d46c5c-947a-49a2-a034-33c4ef0f54a0" />

# 02. Voting

## 1. Voting
- 여러 개의 서로 다른 모델들의 예측 결과를 투표 방식으로 결합하는 방법

## 2. Hard Voting
- 각 모델이 예측한 클래스 중 많이 선택된 것을 최종 예측값으로 결정하는 다수결 방식
- 분류 문제에 주로 사용됨

## 3. Soft Voting
- 각 모델이 예측한 클래스의 확률 또는 예측값을 평균 -> 가장 높은 확률을 가진 클래스 선택
- 분류 뿐 아니라 회귀 문제에도 사용

<img width="842" height="370" alt="image" src="https://github.com/user-attachments/assets/668c39de-dd38-4eaf-98bc-bfbd1202d215" />

# 03. Bagging

## 1. Bagging
<img width="387" height="244" alt="image" src="https://github.com/user-attachments/assets/32952327-e774-42f6-9483-93a384650d32" />

- Bootstrap Aggregating의 줄임말
- 훈련 데이터셋에서 중복을 허용해 샘플링 : Bootstrap
- 동일한 모델을 학습시켜 예측 결과를 통함 : Aggregating

- 데이터의 여러 부트스트랩 샘플 (중복 허용 샘플링) 을 사용해서 각각의 모델을 학습시키고, 그 결과를 회귀 또는 분류 방식으로 결합 -> 예측 분산을 줄이고 안정적 성능 확보
- 사용되는 모델 특성 : 복잡도가 높은 모델 -> 모델은 데이터에 과도하게 반응하는 경향 = 과적합 -> bagging으로 완화시킴

## 2. Bootstrap
- 모집단으로부터 중복을 허용해 표본을 추출하는 방식
- 전체 훈련 데이터셋에서 중복 허용 샘플링을 통해 여러 개의 훈련 데이터 셋을 만듦
<img width="598" height="132" alt="image" src="https://github.com/user-attachments/assets/8557b921-6712-45b9-b2c8-adb8211f9885" />

- 전체 데이터의 약 36.8%는 어떤 부트스트랩 샘플에도 포함되지 않음
- 부트스트랩 과정에서 한 번도 뽑히지 않은 데이터 : Out-of-Bag (OOB) 샘플
- OBB를 활용해 해당 모델의 전체 성능 평가 : Out-of-Bag (OOB) 평가

## 3. Random Forest

<img width="570" height="245" alt="image" src="https://github.com/user-attachments/assets/4e5cfe82-3d0c-4e00-98be-555f9bfde59e" />

- bagging의 대표적 알고리즘
- 여러 개의 Decision Tree를 학습 시키고, 그 결과를 종합해서 최종 예측 수행
- 각 트리는 서로 다른 데이터 샘플과 랜덤하게 선택된 특성을 사용해 학습
- 결과를 집계해 최종 예측

* 정리
* bagging = 다수의 복잡한 모델 + 데이터 샘플링 + 결과 집계
* 목적 = 분산 감소 및 성능 안정화
* Random Forest = Bagging + Decision Tree + 랜덤 특성 선택
* 과적함 위험이 큰 모델에 특히 효과적

### 1) Bootstrap - Decision Tree
- 각 Tree는 Bootstrap sampling 을 통해 만들어진 서로 다른 데이터 셋을 학습함

### 2) Random Feature Selection
- 일반 Decision Tree : 모든 피처를 대상으로 정보이득 / 지니 지수가 가장 좋은 피처 선택
- Random Forest : 무작위로 선택딘 일부 피처만 고려해 분기 생성

### 3) Aggregation
- 학습된 여러 Tree의 예측을 종합해 최종 예측 수행

### 특징
- Random Forest는 개별 Decision Tree보다 일반화 성능이 뛰어남
- 과적합을 방지하면서 복잡한 데이터 패턴도 잘 포착 가능

### 4) Out-od-Bag Evaluvtion
- 각 Tree는 Bootstrap Sampling으로 만든 일부 데이터만 학습 -> 학습에 사용되지 않은 나머지 데이터 (OOB) 를 검증 데이터로 활용해 모델 성능을 평가함
- 장점
  - 별도의 검증 데이터셋 불필요
  - 교차 검증 수준의 신뢰도 있는 성능 평가 가능
  - 데이터가 부족할 때 특히 유용
- Feature Importace
  - 학습된 모델 해석 가능 : 각 피처가 예측에 기여한 정도를 측정
  - 계산 방법 : 전체 Tree에서 각 변수의 평균 불순도 감소 정도를 기반으로 변수 중요도 산출
  - 주의점 : 클래스가 ㅁ낳을수록 변수 중요도가 과대평가될 수 있음, 상관관게가 높은 변수 간에는 왜곡 발생 가능
  <img width="667" height="216" alt="image" src="https://github.com/user-attachments/assets/450e4ff2-beff-4e29-828c-215352433521" />

<img width="505" height="194" alt="image" src="https://github.com/user-attachments/assets/5e3410ef-4b10-
44c2-b255-bf55bc79e4d1" />

# 04. Boosting
- 여러 개의 약한 학습기를 순차적으로 학습시켜 강한 학습기를 만드는 방법
- 각 단계에서는 이전 모델이 잘못 예측한 데이터에 더 큰 가중치를 부여해 이후 모델이 올바르게 예측할 수 있도록 학습
- 모델이 점점 복잡해지면서 편향이 줄어들고, 과소적합 문제 완화

<img width="580" height="190" alt="image" src="https://github.com/user-attachments/assets/7f029f41-7d4d-4287-bf24-4a506203aea7" />

## 1. ADaBoost (Adaptive Boost
<img width="552" height="220" alt="image" src="https://github.com/user-attachments/assets/6eb4bc59-ef5f-
4b51-8dbe-76d3dee4b522" />

<img width="692" height="210" alt="image" src="https://github.com/user-attachments/assets/1536a215-1e8c-45a3-b111-5b488fff56f8" />

- 학습기를 업데이트하는 과정에서 예측값이 얼마나 틀렸는지가 아니라 맞았는디 틀렸는지만 보고 가중치를 조절함
-> 유연성이 떨어지며 연속적 회귀 문제에 적용하기 어려움 + MSE, 로그 손실과 같은 다양한 손실 함수 사용 또한 어려움

## 2. GBM (Gradient Boosting)
- 경사하강법을 활용해 손실 함수의 최소화하는 방향으로 다음 학습기를 학습
- 실제 값과 예측 값 사이의 오차 줄이기 = 손실함수 최소화 = 손실함수 Gradient 최소화하는 가중치 값을 구함

<img width="324" height="174" alt="image" src="https://github.com/user-attachments/assets/e2455551-b96d-4fe1-ad1e-75999cd02133" />

<img width="698" height="208" alt="image" src="https://github.com/user-attachments/assets/549f2c7b-5dec-4e70-a5b3-8872052e724f" />

<img width="531" height="92" alt="image" src="https://github.com/user-attachments/assets/2882c021-52ff-4a18-8b10-747a2c8b0893" />

- 손실함수를 최소화하기 위한 계산을 순차적으로 하다 보니 속도가 느림
- 과적합의 가능성이 크며, 대용량이나 고차원 데이터에서 계산 비용이 급증함

## 3. XGBoost
- GBM 기반으로하면서 성능과 효율성 모두 향상시킴
